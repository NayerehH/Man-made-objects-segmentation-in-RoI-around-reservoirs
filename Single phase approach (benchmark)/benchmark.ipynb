{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calling libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation Models: using `keras` framework.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'tf.keras'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import albumentations as A\n",
    "import glob\n",
    "import segmentation_models as sm\n",
    "from sklearn.metrics import classification_report\n",
    "sm.set_framework('tf.keras')\n",
    "sm.framework()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% |  0% |\n",
      "|  1 |  1% |  0% |\n",
      "|  2 |  0% |  0% |\n",
      "|  3 |  0% |  0% |\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import GPUtil\n",
    "import psutil\n",
    "import tensorflow as tf\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "GPUtil.showUtilization()\n",
    "psutil.cpu_percent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = './bench_data/' \n",
    "\n",
    "x_train_dir = os.path.join(DATA_DIR, 'train_rgb/')\n",
    "y_train_dir = os.path.join(DATA_DIR, 'train_anno/')\n",
    "\n",
    "x_valid_dir = os.path.join(DATA_DIR, 'test_rgb/')\n",
    "y_valid_dir = os.path.join(DATA_DIR, 'test_anno/')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Necessary functions and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "\n",
    "    \n",
    "    CLASSES = ['non', 'reservoir','manmade']\n",
    "    \n",
    "    def __init__(\n",
    "            self, \n",
    "            images_dir, \n",
    "            masks_dir, \n",
    "            classes=None, \n",
    "            augmentation=None, \n",
    "            preprocessing=None,\n",
    "    ):\n",
    "        self.ids = os.listdir(images_dir)\n",
    "        self.images_fps = [os.path.join(images_dir, image_id) for image_id in self.ids]\n",
    "        self.masks_fps = [os.path.join(masks_dir, image_id) for image_id in self.ids]\n",
    "        \n",
    "        self.class_values = [self.CLASSES.index(cls.lower()) for cls in classes]\n",
    "        \n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        \n",
    "        image = cv2.imread(self.images_fps[i])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(self.masks_fps[i], 0)\n",
    "        \n",
    "        masks = [(mask == v) for v in self.class_values]\n",
    "        mask = np.stack(masks, axis=-1).astype('float')\n",
    "        \n",
    "        if mask.shape[-1] != 1:\n",
    "            background = 1 - mask.sum(axis=-1, keepdims=True)\n",
    "            mask = np.concatenate((mask, background), axis=-1)\n",
    "        \n",
    "        if self.augmentation:\n",
    "            sample = self.augmentation(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "        \n",
    "        if self.preprocessing:\n",
    "            sample = self.preprocessing(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "            \n",
    "        return image, mask\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "    \n",
    "    \n",
    "class Dataloder(tf.keras.utils.Sequence):\n",
    "\n",
    "    def __init__(self, dataset, batch_size=1, shuffle=False):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indexes = np.arange(len(dataset))\n",
    "\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        \n",
    "        start = i * self.batch_size\n",
    "        stop = (i + 1) * self.batch_size\n",
    "        data = []\n",
    "        for j in range(start, stop):\n",
    "            data.append(self.dataset[j])\n",
    "        \n",
    "        batch = [np.stack(samples, axis=0) for samples in zip(*data)]\n",
    "        \n",
    "        return batch\n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.indexes) // self.batch_size\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        \n",
    "        if self.shuffle:\n",
    "            self.indexes = np.random.permutation(self.indexes)  \n",
    "            \n",
    "\n",
    "            \n",
    "            \n",
    "def round_clip_0_1(x, **kwargs):\n",
    "    return x.round().clip(0, 1)\n",
    "\n",
   
    "def get_training_augmentation():\n",
    "    train_transform = [\n",
    "\n",
    "        A.HorizontalFlip(p=0.7),\n",
    "        A.VerticalFlip(p=0.7),\n",
    "\n",
    "\n",
    "        A.Lambda(mask=round_clip_0_1)\n",
    "    ]\n",
    "    return A.Compose(train_transform)\n",
    "\n",
    "def denormalize(x):\n",
    "    x_max = np.percentile(x, 98)\n",
    "    x_min = np.percentile(x, 2)    \n",
    "    x = (x - x_min) / (x_max - x_min)\n",
    "    x = x.clip(0, 1)\n",
    "    return x\n",
    "\n",
    "def get_validation_augmentation():\n",
    "    \n",
    "    test_transform = [\n",
    "        A.PadIfNeeded(384, 384)\n",
    "    ]\n",
    "    return A.Compose(test_transform)\n",
    "\n",
    "def get_preprocessing(preprocessing_fn):\n",
    "\n",
    "    \n",
    "    _transform = [\n",
    "        A.Lambda(image=preprocessing_fn),\n",
    "    ]\n",
    "    return A.Compose(_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BACKBONE = 'vgg16'\n",
    "BATCH_SIZE = 8\n",
    "CLASSES = [ 'reservoir', 'manmade']\n",
    "LR = 0.0001\n",
    "EPOCHS = 150\n",
    "\n",
    "preprocess_input = sm.get_preprocessing(BACKBONE)\n",
    "\n",
    "n_classes = len(CLASSES) + 1 \n",
    "activation = 'softmax'\n",
    "\n",
    "#create model                                            \n",
    "model = sm.Unet(BACKBONE, classes=n_classes, activation=activation,encoder_weights='imagenet',decoder_filters=(256,128, 64, 32, 16))\n",
    "\n",
    "\n",
    "from tensorflow import keras\n",
    "optim = keras.optimizers.Adam(LR)\n",
    "\n",
    "dice_loss = sm.losses.DiceLoss(class_weights=np.array([1,1,1]))\n",
    "focal_loss = sm.losses.BinaryFocalLoss() if n_classes == 1 else sm.losses.CategoricalFocalLoss()\n",
    "total_loss = dice_loss + (1 * focal_loss)\n",
    "\n",
    "metrics = [sm.metrics.Precision(threshold=0.5),sm.metrics.Recall(threshold=0.5), sm.metrics.FScore(threshold=0.5)]\n",
    "\n",
    "model.compile(optim, total_loss, metrics)\n",
    "\n",
    "model_name =  './benchmark_unet.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset(\n",
    "    x_train_dir, \n",
    "    y_train_dir, \n",
    "    classes=CLASSES, \n",
    "    augmentation=get_training_augmentation(),\n",
    "    preprocessing=get_preprocessing(preprocess_input),\n",
    ")\n",
    "\n",
    "valid_dataset = Dataset(\n",
    "    x_valid_dir, \n",
    "    y_valid_dir, \n",
    "    classes=CLASSES, \n",
    "    augmentation=get_validation_augmentation(),\n",
    "    preprocessing=get_preprocessing(preprocess_input),\n",
    ")\n",
    "\n",
    "train_dataloader = Dataloder(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_dataloader = Dataloder(valid_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "assert train_dataloader[0][0].shape == (BATCH_SIZE,384, 384,3) \n",
    "assert train_dataloader[0][1].shape == (BATCH_SIZE,384, 384, n_classes)\n",
    "\n",
    "\n",
    "callbacks = [                         \n",
    "    keras.callbacks.ModelCheckpoint(model_name, save_weights_only=True, save_best_only=True, mode='min'),\n",
    "    keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-7, verbose=True),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nayereh/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:7: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "1474/1474 [==============================] - 653s 436ms/step - loss: 0.4626 - precision: 0.6062 - recall: 0.7091 - f1-score: 0.5990 - val_loss: 0.6055 - val_precision: 0.6502 - val_recall: 0.9016 - val_f1-score: 0.6203 - lr: 1.0000e-04\n",
      "Epoch 2/150\n",
      "1474/1474 [==============================] - 636s 431ms/step - loss: 0.3795 - precision: 0.6507 - recall: 0.7500 - f1-score: 0.6522 - val_loss: 0.6122 - val_precision: 0.6705 - val_recall: 0.8888 - val_f1-score: 0.6270 - lr: 1.0000e-04\n",
      "Epoch 3/150\n",
      "1474/1474 [==============================] - 638s 433ms/step - loss: 0.3599 - precision: 0.6642 - recall: 0.7650 - f1-score: 0.6674 - val_loss: 0.5901 - val_precision: 0.6890 - val_recall: 0.9092 - val_f1-score: 0.6606 - lr: 1.0000e-04\n",
      "Epoch 4/150\n",
      "1474/1474 [==============================] - 645s 437ms/step - loss: 0.3440 - precision: 0.6763 - recall: 0.7755 - f1-score: 0.6801 - val_loss: 0.5850 - val_precision: 0.6867 - val_recall: 0.9135 - val_f1-score: 0.6616 - lr: 1.0000e-04\n",
      "Epoch 5/150\n",
      "1474/1474 [==============================] - 641s 435ms/step - loss: 0.3385 - precision: 0.6808 - recall: 0.7788 - f1-score: 0.6849 - val_loss: 0.5868 - val_precision: 0.6708 - val_recall: 0.9192 - val_f1-score: 0.6493 - lr: 1.0000e-04\n",
      "Epoch 6/150\n",
      "1474/1474 [==============================] - 634s 430ms/step - loss: 0.3312 - precision: 0.6870 - recall: 0.7842 - f1-score: 0.6906 - val_loss: 0.5917 - val_precision: 0.6706 - val_recall: 0.9036 - val_f1-score: 0.6349 - lr: 1.0000e-04\n",
      "Epoch 7/150\n",
      "1474/1474 [==============================] - 640s 434ms/step - loss: 0.3286 - precision: 0.6886 - recall: 0.7860 - f1-score: 0.6927 - val_loss: 0.5901 - val_precision: 0.7273 - val_recall: 0.9060 - val_f1-score: 0.6925 - lr: 1.0000e-04\n",
      "Epoch 8/150\n",
      "1474/1474 [==============================] - 640s 434ms/step - loss: 0.3233 - precision: 0.6943 - recall: 0.7891 - f1-score: 0.6973 - val_loss: 0.5913 - val_precision: 0.6985 - val_recall: 0.9043 - val_f1-score: 0.6636 - lr: 1.0000e-04\n",
      "Epoch 9/150\n",
      "1474/1474 [==============================] - 646s 438ms/step - loss: 0.3174 - precision: 0.6989 - recall: 0.7922 - f1-score: 0.7023 - val_loss: 0.5798 - val_precision: 0.7190 - val_recall: 0.9120 - val_f1-score: 0.6857 - lr: 1.0000e-04\n",
      "Epoch 10/150\n",
      "1474/1474 [==============================] - 645s 437ms/step - loss: 0.3115 - precision: 0.7084 - recall: 0.7909 - f1-score: 0.7086 - val_loss: 0.5806 - val_precision: 0.7519 - val_recall: 0.9135 - val_f1-score: 0.7203 - lr: 1.0000e-04\n",
      "Epoch 11/150\n",
      "1474/1474 [==============================] - 638s 433ms/step - loss: 0.3049 - precision: 0.7178 - recall: 0.7931 - f1-score: 0.7147 - val_loss: 0.5896 - val_precision: 0.7818 - val_recall: 0.9058 - val_f1-score: 0.7407 - lr: 1.0000e-04\n",
      "Epoch 12/150\n",
      "1474/1474 [==============================] - 636s 432ms/step - loss: 0.2984 - precision: 0.7271 - recall: 0.7942 - f1-score: 0.7202 - val_loss: 0.5785 - val_precision: 0.7776 - val_recall: 0.9154 - val_f1-score: 0.7456 - lr: 1.0000e-04\n",
      "Epoch 13/150\n",
      "1474/1474 [==============================] - 638s 433ms/step - loss: 0.2925 - precision: 0.7347 - recall: 0.7947 - f1-score: 0.7264 - val_loss: 0.5866 - val_precision: 0.7925 - val_recall: 0.9071 - val_f1-score: 0.7536 - lr: 1.0000e-04\n",
      "Epoch 14/150\n",
      "1474/1474 [==============================] - 637s 432ms/step - loss: 0.2918 - precision: 0.7339 - recall: 0.7968 - f1-score: 0.7275 - val_loss: 0.5785 - val_precision: 0.7695 - val_recall: 0.9142 - val_f1-score: 0.7364 - lr: 1.0000e-04\n",
      "Epoch 15/150\n",
      "1474/1474 [==============================] - 637s 432ms/step - loss: 0.2848 - precision: 0.7433 - recall: 0.7991 - f1-score: 0.7334 - val_loss: 0.5792 - val_precision: 0.7980 - val_recall: 0.9153 - val_f1-score: 0.7636 - lr: 1.0000e-04\n",
      "Epoch 16/150\n",
      "1474/1474 [==============================] - 637s 432ms/step - loss: 0.2810 - precision: 0.7483 - recall: 0.8003 - f1-score: 0.7380 - val_loss: 0.5751 - val_precision: 0.8123 - val_recall: 0.9163 - val_f1-score: 0.7769 - lr: 1.0000e-04\n",
      "Epoch 17/150\n",
      "1474/1474 [==============================] - 632s 429ms/step - loss: 0.2783 - precision: 0.7537 - recall: 0.7997 - f1-score: 0.7406 - val_loss: 0.5768 - val_precision: 0.7877 - val_recall: 0.9175 - val_f1-score: 0.7561 - lr: 1.0000e-04\n",
      "Epoch 18/150\n",
      "1474/1474 [==============================] - 635s 431ms/step - loss: 0.2759 - precision: 0.7529 - recall: 0.8029 - f1-score: 0.7409 - val_loss: 0.5804 - val_precision: 0.8153 - val_recall: 0.9151 - val_f1-score: 0.7808 - lr: 1.0000e-04\n",
      "Epoch 19/150\n",
      "1474/1474 [==============================] - 637s 432ms/step - loss: 0.2743 - precision: 0.7580 - recall: 0.8026 - f1-score: 0.7436 - val_loss: 0.5887 - val_precision: 0.8762 - val_recall: 0.9051 - val_f1-score: 0.8219 - lr: 1.0000e-04\n",
      "Epoch 20/150\n",
      "1474/1474 [==============================] - 636s 431ms/step - loss: 0.2688 - precision: 0.7593 - recall: 0.8084 - f1-score: 0.7473 - val_loss: 0.5769 - val_precision: 0.8053 - val_recall: 0.9213 - val_f1-score: 0.7749 - lr: 1.0000e-04\n",
      "Epoch 21/150\n",
      "1474/1474 [==============================] - ETA: 0s - loss: 0.2707 - precision: 0.7580 - recall: 0.8066 - f1-score: 0.7463\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "1474/1474 [==============================] - 631s 428ms/step - loss: 0.2707 - precision: 0.7580 - recall: 0.8066 - f1-score: 0.7463 - val_loss: 0.5780 - val_precision: 0.8610 - val_recall: 0.9140 - val_f1-score: 0.8156 - lr: 1.0000e-04\n",
      "Epoch 22/150\n",
      "1474/1474 [==============================] - 637s 432ms/step - loss: 0.2516 - precision: 0.7759 - recall: 0.8184 - f1-score: 0.7629 - val_loss: 0.5755 - val_precision: 0.8655 - val_recall: 0.9151 - val_f1-score: 0.8245 - lr: 2.0000e-05\n",
      "Epoch 23/150\n",
      "1474/1474 [==============================] - 633s 429ms/step - loss: 0.2437 - precision: 0.7850 - recall: 0.8236 - f1-score: 0.7703 - val_loss: 0.5712 - val_precision: 0.8512 - val_recall: 0.9198 - val_f1-score: 0.8168 - lr: 2.0000e-05\n",
      "Epoch 24/150\n",
      "1474/1474 [==============================] - 638s 433ms/step - loss: 0.2403 - precision: 0.7891 - recall: 0.8258 - f1-score: 0.7757 - val_loss: 0.5721 - val_precision: 0.8732 - val_recall: 0.9175 - val_f1-score: 0.8329 - lr: 2.0000e-05\n",
      "Epoch 25/150\n",
      "1474/1474 [==============================] - 642s 435ms/step - loss: 0.2379 - precision: 0.7904 - recall: 0.8276 - f1-score: 0.7766 - val_loss: 0.5705 - val_precision: 0.8635 - val_recall: 0.9192 - val_f1-score: 0.8260 - lr: 2.0000e-05\n",
      "Epoch 26/150\n",
      "1474/1474 [==============================] - 642s 436ms/step - loss: 0.2341 - precision: 0.7954 - recall: 0.8298 - f1-score: 0.7814 - val_loss: 0.5755 - val_precision: 0.8791 - val_recall: 0.9147 - val_f1-score: 0.8350 - lr: 2.0000e-05\n",
      "Epoch 27/150\n",
      "1474/1474 [==============================] - 633s 429ms/step - loss: 0.2336 - precision: 0.7946 - recall: 0.8308 - f1-score: 0.7810 - val_loss: 0.5706 - val_precision: 0.8715 - val_recall: 0.9196 - val_f1-score: 0.8332 - lr: 2.0000e-05\n",
      "Epoch 28/150\n",
      "1474/1474 [==============================] - 641s 435ms/step - loss: 0.2308 - precision: 0.7984 - recall: 0.8329 - f1-score: 0.7843 - val_loss: 0.5716 - val_precision: 0.8809 - val_recall: 0.9179 - val_f1-score: 0.8399 - lr: 2.0000e-05\n",
      "Epoch 29/150\n",
      "1474/1474 [==============================] - 640s 434ms/step - loss: 0.2279 - precision: 0.8036 - recall: 0.8317 - f1-score: 0.7873 - val_loss: 0.5699 - val_precision: 0.8661 - val_recall: 0.9208 - val_f1-score: 0.8295 - lr: 2.0000e-05\n",
      "Epoch 30/150\n",
      "1474/1474 [==============================] - 638s 433ms/step - loss: 0.2280 - precision: 0.8010 - recall: 0.8344 - f1-score: 0.7865 - val_loss: 0.5696 - val_precision: 0.8747 - val_recall: 0.9200 - val_f1-score: 0.8363 - lr: 2.0000e-05\n",
      "Epoch 31/150\n",
      "1474/1474 [==============================] - 636s 431ms/step - loss: 0.2257 - precision: 0.8052 - recall: 0.8332 - f1-score: 0.7889 - val_loss: 0.5702 - val_precision: 0.8816 - val_recall: 0.9197 - val_f1-score: 0.8414 - lr: 2.0000e-05\n",
      "Epoch 32/150\n",
      "1474/1474 [==============================] - 638s 433ms/step - loss: 0.2232 - precision: 0.8067 - recall: 0.8360 - f1-score: 0.7909 - val_loss: 0.5705 - val_precision: 0.8829 - val_recall: 0.9187 - val_f1-score: 0.8421 - lr: 2.0000e-05\n",
      "Epoch 33/150\n",
      "1474/1474 [==============================] - 640s 434ms/step - loss: 0.2230 - precision: 0.8072 - recall: 0.8363 - f1-score: 0.7922 - val_loss: 0.5716 - val_precision: 0.8842 - val_recall: 0.9181 - val_f1-score: 0.8423 - lr: 2.0000e-05\n",
      "Epoch 34/150\n",
      "1474/1474 [==============================] - 640s 434ms/step - loss: 0.2214 - precision: 0.8085 - recall: 0.8382 - f1-score: 0.7938 - val_loss: 0.5734 - val_precision: 0.8927 - val_recall: 0.9166 - val_f1-score: 0.8488 - lr: 2.0000e-05\n",
      "Epoch 35/150\n",
      "1474/1474 [==============================] - ETA: 0s - loss: 0.2187 - precision: 0.8098 - recall: 0.8421 - f1-score: 0.7959\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 3.999999898951501e-06.\n",
      "1474/1474 [==============================] - 636s 432ms/step - loss: 0.2187 - precision: 0.8098 - recall: 0.8421 - f1-score: 0.7959 - val_loss: 0.5709 - val_precision: 0.8764 - val_recall: 0.9175 - val_f1-score: 0.8363 - lr: 2.0000e-05\n",
      "Epoch 36/150\n",
      "1474/1474 [==============================] - 636s 431ms/step - loss: 0.2142 - precision: 0.8160 - recall: 0.8421 - f1-score: 0.8006 - val_loss: 0.5699 - val_precision: 0.8875 - val_recall: 0.9193 - val_f1-score: 0.8467 - lr: 4.0000e-06\n",
      "Epoch 37/150\n",
      "1474/1474 [==============================] - 640s 434ms/step - loss: 0.2133 - precision: 0.8181 - recall: 0.8428 - f1-score: 0.8022 - val_loss: 0.5705 - val_precision: 0.8929 - val_recall: 0.9184 - val_f1-score: 0.8501 - lr: 4.0000e-06\n",
      "Epoch 38/150\n",
      "1474/1474 [==============================] - 640s 434ms/step - loss: 0.2120 - precision: 0.8197 - recall: 0.8432 - f1-score: 0.8038 - val_loss: 0.5700 - val_precision: 0.8903 - val_recall: 0.9194 - val_f1-score: 0.8488 - lr: 4.0000e-06\n",
      "Epoch 39/150\n",
      "1474/1474 [==============================] - 638s 433ms/step - loss: 0.2114 - precision: 0.8184 - recall: 0.8451 - f1-score: 0.8033 - val_loss: 0.5704 - val_precision: 0.8928 - val_recall: 0.9191 - val_f1-score: 0.8508 - lr: 4.0000e-06\n",
      "Epoch 40/150\n",
      "1474/1474 [==============================] - ETA: 0s - loss: 0.2099 - precision: 0.8207 - recall: 0.8460 - f1-score: 0.8055\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 7.999999979801942e-07.\n",
      "1474/1474 [==============================] - 635s 431ms/step - loss: 0.2099 - precision: 0.8207 - recall: 0.8460 - f1-score: 0.8055 - val_loss: 0.5705 - val_precision: 0.8910 - val_recall: 0.9186 - val_f1-score: 0.8493 - lr: 4.0000e-06\n",
      "Epoch 41/150\n",
      "1474/1474 [==============================] - 638s 433ms/step - loss: 0.2087 - precision: 0.8213 - recall: 0.8465 - f1-score: 0.8067 - val_loss: 0.5704 - val_precision: 0.8912 - val_recall: 0.9190 - val_f1-score: 0.8499 - lr: 8.0000e-07\n",
      "Epoch 42/150\n",
      "1474/1474 [==============================] - 641s 434ms/step - loss: 0.2081 - precision: 0.8212 - recall: 0.8474 - f1-score: 0.8069 - val_loss: 0.5705 - val_precision: 0.8936 - val_recall: 0.9188 - val_f1-score: 0.8514 - lr: 8.0000e-07\n",
      "Epoch 43/150\n",
      "1474/1474 [==============================] - 641s 435ms/step - loss: 0.2085 - precision: 0.8226 - recall: 0.8458 - f1-score: 0.8071 - val_loss: 0.5704 - val_precision: 0.8942 - val_recall: 0.9186 - val_f1-score: 0.8518 - lr: 8.0000e-07\n",
      "Epoch 44/150\n",
      "1474/1474 [==============================] - 637s 432ms/step - loss: 0.2078 - precision: 0.8225 - recall: 0.8464 - f1-score: 0.8068 - val_loss: 0.5703 - val_precision: 0.8917 - val_recall: 0.9195 - val_f1-score: 0.8503 - lr: 8.0000e-07\n",
      "Epoch 45/150\n",
      "1474/1474 [==============================] - ETA: 0s - loss: 0.2077 - precision: 0.8219 - recall: 0.8470 - f1-score: 0.8072\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 1.600000018697756e-07.\n",
      "1474/1474 [==============================] - 634s 430ms/step - loss: 0.2077 - precision: 0.8219 - recall: 0.8470 - f1-score: 0.8072 - val_loss: 0.5705 - val_precision: 0.8942 - val_recall: 0.9185 - val_f1-score: 0.8511 - lr: 8.0000e-07\n",
      "Epoch 46/150\n",
      "1474/1474 [==============================] - 637s 432ms/step - loss: 0.2079 - precision: 0.8226 - recall: 0.8465 - f1-score: 0.8067 - val_loss: 0.5709 - val_precision: 0.8968 - val_recall: 0.9180 - val_f1-score: 0.8529 - lr: 1.6000e-07\n",
      "Epoch 47/150\n",
      "1474/1474 [==============================] - 639s 434ms/step - loss: 0.2067 - precision: 0.8231 - recall: 0.8476 - f1-score: 0.8081 - val_loss: 0.5699 - val_precision: 0.8898 - val_recall: 0.9195 - val_f1-score: 0.8486 - lr: 1.6000e-07\n",
      "Epoch 48/150\n",
      "1474/1474 [==============================] - 640s 434ms/step - loss: 0.2071 - precision: 0.8239 - recall: 0.8477 - f1-score: 0.8085 - val_loss: 0.5705 - val_precision: 0.8933 - val_recall: 0.9187 - val_f1-score: 0.8508 - lr: 1.6000e-07\n",
      "Epoch 49/150\n",
      "1474/1474 [==============================] - 638s 433ms/step - loss: 0.2082 - precision: 0.8213 - recall: 0.8465 - f1-score: 0.8060 - val_loss: 0.5700 - val_precision: 0.8906 - val_recall: 0.9195 - val_f1-score: 0.8495 - lr: 1.6000e-07\n",
      "Epoch 50/150\n",
      "1474/1474 [==============================] - ETA: 0s - loss: 0.2071 - precision: 0.8238 - recall: 0.8466 - f1-score: 0.8080\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "1474/1474 [==============================] - 644s 437ms/step - loss: 0.2071 - precision: 0.8238 - recall: 0.8466 - f1-score: 0.8080 - val_loss: 0.5702 - val_precision: 0.8920 - val_recall: 0.9189 - val_f1-score: 0.8501 - lr: 1.6000e-07\n",
      "Epoch 51/150\n",
      "1474/1474 [==============================] - 637s 432ms/step - loss: 0.2074 - precision: 0.8229 - recall: 0.8473 - f1-score: 0.8075 - val_loss: 0.5699 - val_precision: 0.8899 - val_recall: 0.9196 - val_f1-score: 0.8487 - lr: 1.0000e-07\n",
      "Epoch 52/150\n",
      "1474/1474 [==============================] - 643s 436ms/step - loss: 0.2079 - precision: 0.8228 - recall: 0.8465 - f1-score: 0.8070 - val_loss: 0.5705 - val_precision: 0.8947 - val_recall: 0.9186 - val_f1-score: 0.8520 - lr: 1.0000e-07\n",
      "Epoch 53/150\n",
      "1474/1474 [==============================] - 640s 434ms/step - loss: 0.2077 - precision: 0.8221 - recall: 0.8469 - f1-score: 0.8076 - val_loss: 0.5703 - val_precision: 0.8916 - val_recall: 0.9190 - val_f1-score: 0.8497 - lr: 1.0000e-07\n",
      "Epoch 54/150\n",
      "1474/1474 [==============================] - 638s 433ms/step - loss: 0.2075 - precision: 0.8226 - recall: 0.8475 - f1-score: 0.8073 - val_loss: 0.5700 - val_precision: 0.8915 - val_recall: 0.9195 - val_f1-score: 0.8502 - lr: 1.0000e-07\n",
      "Epoch 55/150\n",
      "1474/1474 [==============================] - 638s 433ms/step - loss: 0.2073 - precision: 0.8245 - recall: 0.8468 - f1-score: 0.8087 - val_loss: 0.5702 - val_precision: 0.8917 - val_recall: 0.9191 - val_f1-score: 0.8499 - lr: 1.0000e-07\n",
      "Epoch 56/150\n",
      "1474/1474 [==============================] - 634s 430ms/step - loss: 0.2072 - precision: 0.8236 - recall: 0.8469 - f1-score: 0.8083 - val_loss: 0.5711 - val_precision: 0.8976 - val_recall: 0.9179 - val_f1-score: 0.8537 - lr: 1.0000e-07\n",
      "Epoch 57/150\n",
      "1474/1474 [==============================] - 633s 429ms/step - loss: 0.2075 - precision: 0.8214 - recall: 0.8469 - f1-score: 0.8064 - val_loss: 0.5708 - val_precision: 0.8971 - val_recall: 0.9183 - val_f1-score: 0.8537 - lr: 1.0000e-07\n",
      "Epoch 58/150\n",
      "1474/1474 [==============================] - 635s 430ms/step - loss: 0.2074 - precision: 0.8226 - recall: 0.8475 - f1-score: 0.8076 - val_loss: 0.5702 - val_precision: 0.8918 - val_recall: 0.9187 - val_f1-score: 0.8497 - lr: 1.0000e-07\n",
      "Epoch 59/150\n",
      "1474/1474 [==============================] - 633s 430ms/step - loss: 0.2077 - precision: 0.8233 - recall: 0.8463 - f1-score: 0.8073 - val_loss: 0.5705 - val_precision: 0.8942 - val_recall: 0.9186 - val_f1-score: 0.8517 - lr: 1.0000e-07\n",
      "Epoch 60/150\n",
      "1474/1474 [==============================] - 636s 431ms/step - loss: 0.2069 - precision: 0.8230 - recall: 0.8481 - f1-score: 0.8078 - val_loss: 0.5702 - val_precision: 0.8925 - val_recall: 0.9192 - val_f1-score: 0.8509 - lr: 1.0000e-07\n",
      "Epoch 61/150\n",
      "1474/1474 [==============================] - 638s 432ms/step - loss: 0.2077 - precision: 0.8225 - recall: 0.8468 - f1-score: 0.8069 - val_loss: 0.5700 - val_precision: 0.8904 - val_recall: 0.9195 - val_f1-score: 0.8491 - lr: 1.0000e-07\n",
      "Epoch 62/150\n",
      "1474/1474 [==============================] - 636s 431ms/step - loss: 0.2065 - precision: 0.8234 - recall: 0.8481 - f1-score: 0.8095 - val_loss: 0.5699 - val_precision: 0.8902 - val_recall: 0.9198 - val_f1-score: 0.8495 - lr: 1.0000e-07\n",
      "Epoch 63/150\n",
      "1474/1474 [==============================] - 638s 432ms/step - loss: 0.2070 - precision: 0.8233 - recall: 0.8469 - f1-score: 0.8085 - val_loss: 0.5707 - val_precision: 0.8957 - val_recall: 0.9184 - val_f1-score: 0.8526 - lr: 1.0000e-07\n",
      "Epoch 64/150\n",
      "1474/1474 [==============================] - 638s 433ms/step - loss: 0.2074 - precision: 0.8230 - recall: 0.8474 - f1-score: 0.8072 - val_loss: 0.5708 - val_precision: 0.8964 - val_recall: 0.9180 - val_f1-score: 0.8524 - lr: 1.0000e-07\n",
      "Epoch 65/150\n",
      "1474/1474 [==============================] - 636s 432ms/step - loss: 0.2073 - precision: 0.8227 - recall: 0.8477 - f1-score: 0.8081 - val_loss: 0.5709 - val_precision: 0.8968 - val_recall: 0.9180 - val_f1-score: 0.8530 - lr: 1.0000e-07\n",
      "Epoch 66/150\n",
      "1474/1474 [==============================] - 636s 431ms/step - loss: 0.2071 - precision: 0.8243 - recall: 0.8467 - f1-score: 0.8083 - val_loss: 0.5708 - val_precision: 0.8956 - val_recall: 0.9183 - val_f1-score: 0.8525 - lr: 1.0000e-07\n",
      "Epoch 67/150\n",
      "1474/1474 [==============================] - 634s 430ms/step - loss: 0.2069 - precision: 0.8230 - recall: 0.8476 - f1-score: 0.8079 - val_loss: 0.5704 - val_precision: 0.8933 - val_recall: 0.9188 - val_f1-score: 0.8509 - lr: 1.0000e-07\n",
      "Epoch 68/150\n",
      "1474/1474 [==============================] - 634s 430ms/step - loss: 0.2067 - precision: 0.8220 - recall: 0.8483 - f1-score: 0.8082 - val_loss: 0.5701 - val_precision: 0.8912 - val_recall: 0.9193 - val_f1-score: 0.8498 - lr: 1.0000e-07\n",
      "Epoch 69/150\n",
      "1474/1474 [==============================] - 644s 437ms/step - loss: 0.2060 - precision: 0.8247 - recall: 0.8480 - f1-score: 0.8095 - val_loss: 0.5702 - val_precision: 0.8922 - val_recall: 0.9193 - val_f1-score: 0.8504 - lr: 1.0000e-07\n",
      "Epoch 70/150\n",
      "1474/1474 [==============================] - 636s 431ms/step - loss: 0.2072 - precision: 0.8231 - recall: 0.8473 - f1-score: 0.8081 - val_loss: 0.5705 - val_precision: 0.8940 - val_recall: 0.9186 - val_f1-score: 0.8514 - lr: 1.0000e-07\n",
      "Epoch 71/150\n",
      "1474/1474 [==============================] - 638s 432ms/step - loss: 0.2074 - precision: 0.8230 - recall: 0.8471 - f1-score: 0.8083 - val_loss: 0.5704 - val_precision: 0.8931 - val_recall: 0.9189 - val_f1-score: 0.8509 - lr: 1.0000e-07\n",
      "Epoch 72/150\n",
      "1474/1474 [==============================] - 639s 434ms/step - loss: 0.2077 - precision: 0.8232 - recall: 0.8463 - f1-score: 0.8078 - val_loss: 0.5704 - val_precision: 0.8938 - val_recall: 0.9188 - val_f1-score: 0.8515 - lr: 1.0000e-07\n",
      "Epoch 73/150\n",
      "1474/1474 [==============================] - 641s 434ms/step - loss: 0.2067 - precision: 0.8229 - recall: 0.8474 - f1-score: 0.8081 - val_loss: 0.5706 - val_precision: 0.8945 - val_recall: 0.9186 - val_f1-score: 0.8519 - lr: 1.0000e-07\n",
      "Epoch 74/150\n",
      "1474/1474 [==============================] - 636s 431ms/step - loss: 0.2068 - precision: 0.8233 - recall: 0.8466 - f1-score: 0.8078 - val_loss: 0.5704 - val_precision: 0.8940 - val_recall: 0.9188 - val_f1-score: 0.8516 - lr: 1.0000e-07\n",
      "Epoch 75/150\n",
      "1474/1474 [==============================] - 636s 432ms/step - loss: 0.2068 - precision: 0.8242 - recall: 0.8477 - f1-score: 0.8090 - val_loss: 0.5704 - val_precision: 0.8943 - val_recall: 0.9189 - val_f1-score: 0.8519 - lr: 1.0000e-07\n",
      "Epoch 76/150\n",
      "1474/1474 [==============================] - 634s 430ms/step - loss: 0.2072 - precision: 0.8225 - recall: 0.8473 - f1-score: 0.8076 - val_loss: 0.5703 - val_precision: 0.8926 - val_recall: 0.9191 - val_f1-score: 0.8506 - lr: 1.0000e-07\n",
      "Epoch 77/150\n",
      "1474/1474 [==============================] - 631s 428ms/step - loss: 0.2067 - precision: 0.8242 - recall: 0.8470 - f1-score: 0.8090 - val_loss: 0.5706 - val_precision: 0.8943 - val_recall: 0.9184 - val_f1-score: 0.8516 - lr: 1.0000e-07\n",
      "Epoch 78/150\n",
      "1474/1474 [==============================] - 637s 432ms/step - loss: 0.2070 - precision: 0.8235 - recall: 0.8473 - f1-score: 0.8079 - val_loss: 0.5705 - val_precision: 0.8942 - val_recall: 0.9189 - val_f1-score: 0.8521 - lr: 1.0000e-07\n",
      "Epoch 79/150\n",
      "1474/1474 [==============================] - 639s 433ms/step - loss: 0.2060 - precision: 0.8245 - recall: 0.8486 - f1-score: 0.8094 - val_loss: 0.5707 - val_precision: 0.8945 - val_recall: 0.9183 - val_f1-score: 0.8515 - lr: 1.0000e-07\n",
      "Epoch 80/150\n",
      "1474/1474 [==============================] - 638s 433ms/step - loss: 0.2066 - precision: 0.8241 - recall: 0.8484 - f1-score: 0.8089 - val_loss: 0.5705 - val_precision: 0.8933 - val_recall: 0.9187 - val_f1-score: 0.8507 - lr: 1.0000e-07\n",
      "Epoch 81/150\n",
      "1474/1474 [==============================] - 640s 434ms/step - loss: 0.2062 - precision: 0.8240 - recall: 0.8479 - f1-score: 0.8086 - val_loss: 0.5704 - val_precision: 0.8936 - val_recall: 0.9187 - val_f1-score: 0.8511 - lr: 1.0000e-07\n",
      "Epoch 82/150\n",
      "1474/1474 [==============================] - 632s 429ms/step - loss: 0.2075 - precision: 0.8248 - recall: 0.8453 - f1-score: 0.8083 - val_loss: 0.5711 - val_precision: 0.8983 - val_recall: 0.9179 - val_f1-score: 0.8544 - lr: 1.0000e-07\n",
      "Epoch 83/150\n",
      "1474/1474 [==============================] - 640s 434ms/step - loss: 0.2071 - precision: 0.8247 - recall: 0.8466 - f1-score: 0.8089 - val_loss: 0.5702 - val_precision: 0.8921 - val_recall: 0.9190 - val_f1-score: 0.8501 - lr: 1.0000e-07\n",
      "Epoch 84/150\n",
      "1474/1474 [==============================] - 635s 430ms/step - loss: 0.2066 - precision: 0.8238 - recall: 0.8476 - f1-score: 0.8083 - val_loss: 0.5705 - val_precision: 0.8954 - val_recall: 0.9186 - val_f1-score: 0.8527 - lr: 1.0000e-07\n",
      "Epoch 85/150\n",
      "1474/1474 [==============================] - 633s 429ms/step - loss: 0.2074 - precision: 0.8219 - recall: 0.8468 - f1-score: 0.8062 - val_loss: 0.5703 - val_precision: 0.8934 - val_recall: 0.9188 - val_f1-score: 0.8509 - lr: 1.0000e-07\n",
      "Epoch 86/150\n",
      "1474/1474 [==============================] - 639s 433ms/step - loss: 0.2073 - precision: 0.8235 - recall: 0.8469 - f1-score: 0.8082 - val_loss: 0.5706 - val_precision: 0.8956 - val_recall: 0.9185 - val_f1-score: 0.8524 - lr: 1.0000e-07\n",
      "Epoch 87/150\n",
      "1474/1474 [==============================] - 638s 432ms/step - loss: 0.2062 - precision: 0.8240 - recall: 0.8476 - f1-score: 0.8085 - val_loss: 0.5703 - val_precision: 0.8940 - val_recall: 0.9190 - val_f1-score: 0.8515 - lr: 1.0000e-07\n",
      "Epoch 88/150\n",
      "1474/1474 [==============================] - 635s 431ms/step - loss: 0.2069 - precision: 0.8242 - recall: 0.8480 - f1-score: 0.8086 - val_loss: 0.5705 - val_precision: 0.8941 - val_recall: 0.9187 - val_f1-score: 0.8515 - lr: 1.0000e-07\n",
      "Epoch 89/150\n",
      "1474/1474 [==============================] - 636s 431ms/step - loss: 0.2063 - precision: 0.8238 - recall: 0.8482 - f1-score: 0.8092 - val_loss: 0.5707 - val_precision: 0.8943 - val_recall: 0.9185 - val_f1-score: 0.8517 - lr: 1.0000e-07\n",
      "Epoch 90/150\n",
      "1474/1474 [==============================] - 639s 434ms/step - loss: 0.2066 - precision: 0.8244 - recall: 0.8481 - f1-score: 0.8091 - val_loss: 0.5704 - val_precision: 0.8930 - val_recall: 0.9189 - val_f1-score: 0.8507 - lr: 1.0000e-07\n",
      "Epoch 91/150\n",
      "1474/1474 [==============================] - 641s 435ms/step - loss: 0.2055 - precision: 0.8256 - recall: 0.8490 - f1-score: 0.8104 - val_loss: 0.5707 - val_precision: 0.8944 - val_recall: 0.9185 - val_f1-score: 0.8516 - lr: 1.0000e-07\n",
      "Epoch 92/150\n",
      "1474/1474 [==============================] - 636s 432ms/step - loss: 0.2072 - precision: 0.8232 - recall: 0.8480 - f1-score: 0.8090 - val_loss: 0.5709 - val_precision: 0.8970 - val_recall: 0.9183 - val_f1-score: 0.8535 - lr: 1.0000e-07\n",
      "Epoch 93/150\n",
      "1474/1474 [==============================] - 640s 434ms/step - loss: 0.2068 - precision: 0.8233 - recall: 0.8467 - f1-score: 0.8084 - val_loss: 0.5708 - val_precision: 0.8963 - val_recall: 0.9184 - val_f1-score: 0.8530 - lr: 1.0000e-07\n",
      "Epoch 94/150\n",
      "1474/1474 [==============================] - 638s 433ms/step - loss: 0.2066 - precision: 0.8229 - recall: 0.8477 - f1-score: 0.8085 - val_loss: 0.5710 - val_precision: 0.8965 - val_recall: 0.9182 - val_f1-score: 0.8530 - lr: 1.0000e-07\n",
      "Epoch 95/150\n",
      "1474/1474 [==============================] - 639s 433ms/step - loss: 0.2080 - precision: 0.8234 - recall: 0.8456 - f1-score: 0.8077 - val_loss: 0.5702 - val_precision: 0.8919 - val_recall: 0.9196 - val_f1-score: 0.8506 - lr: 1.0000e-07\n",
      "Epoch 96/150\n",
      "1474/1474 [==============================] - 639s 433ms/step - loss: 0.2064 - precision: 0.8248 - recall: 0.8478 - f1-score: 0.8093 - val_loss: 0.5703 - val_precision: 0.8915 - val_recall: 0.9191 - val_f1-score: 0.8498 - lr: 1.0000e-07\n",
      "Epoch 97/150\n",
      "1474/1474 [==============================] - 635s 431ms/step - loss: 0.2060 - precision: 0.8257 - recall: 0.8478 - f1-score: 0.8100 - val_loss: 0.5705 - val_precision: 0.8938 - val_recall: 0.9185 - val_f1-score: 0.8511 - lr: 1.0000e-07\n",
      "Epoch 98/150\n",
      "1474/1474 [==============================] - 636s 431ms/step - loss: 0.2067 - precision: 0.8241 - recall: 0.8476 - f1-score: 0.8083 - val_loss: 0.5708 - val_precision: 0.8946 - val_recall: 0.9184 - val_f1-score: 0.8517 - lr: 1.0000e-07\n",
      "Epoch 99/150\n",
      "1474/1474 [==============================] - 640s 434ms/step - loss: 0.2072 - precision: 0.8232 - recall: 0.8469 - f1-score: 0.8074 - val_loss: 0.5703 - val_precision: 0.8935 - val_recall: 0.9191 - val_f1-score: 0.8513 - lr: 1.0000e-07\n",
      "Epoch 100/150\n",
      "1474/1474 [==============================] - 638s 433ms/step - loss: 0.2072 - precision: 0.8239 - recall: 0.8465 - f1-score: 0.8083 - val_loss: 0.5708 - val_precision: 0.8956 - val_recall: 0.9186 - val_f1-score: 0.8526 - lr: 1.0000e-07\n",
      "Epoch 101/150\n",
      "1474/1474 [==============================] - 639s 433ms/step - loss: 0.2057 - precision: 0.8249 - recall: 0.8487 - f1-score: 0.8098 - val_loss: 0.5703 - val_precision: 0.8929 - val_recall: 0.9191 - val_f1-score: 0.8506 - lr: 1.0000e-07\n",
      "Epoch 102/150\n",
      "1474/1474 [==============================] - 638s 433ms/step - loss: 0.2068 - precision: 0.8240 - recall: 0.8467 - f1-score: 0.8087 - val_loss: 0.5703 - val_precision: 0.8925 - val_recall: 0.9193 - val_f1-score: 0.8507 - lr: 1.0000e-07\n",
      "Epoch 103/150\n",
      "1474/1474 [==============================] - 640s 434ms/step - loss: 0.2065 - precision: 0.8237 - recall: 0.8477 - f1-score: 0.8081 - val_loss: 0.5709 - val_precision: 0.8970 - val_recall: 0.9180 - val_f1-score: 0.8529 - lr: 1.0000e-07\n",
      "Epoch 104/150\n",
      "1474/1474 [==============================] - 641s 435ms/step - loss: 0.2066 - precision: 0.8240 - recall: 0.8478 - f1-score: 0.8089 - val_loss: 0.5708 - val_precision: 0.8963 - val_recall: 0.9185 - val_f1-score: 0.8531 - lr: 1.0000e-07\n",
      "Epoch 105/150\n",
      "1474/1474 [==============================] - 641s 435ms/step - loss: 0.2067 - precision: 0.8241 - recall: 0.8473 - f1-score: 0.8088 - val_loss: 0.5705 - val_precision: 0.8933 - val_recall: 0.9189 - val_f1-score: 0.8509 - lr: 1.0000e-07\n",
      "Epoch 106/150\n",
      "1474/1474 [==============================] - 636s 431ms/step - loss: 0.2069 - precision: 0.8231 - recall: 0.8471 - f1-score: 0.8075 - val_loss: 0.5704 - val_precision: 0.8939 - val_recall: 0.9189 - val_f1-score: 0.8515 - lr: 1.0000e-07\n",
      "Epoch 107/150\n",
      "1474/1474 [==============================] - 639s 434ms/step - loss: 0.2067 - precision: 0.8232 - recall: 0.8472 - f1-score: 0.8081 - val_loss: 0.5703 - val_precision: 0.8914 - val_recall: 0.9190 - val_f1-score: 0.8496 - lr: 1.0000e-07\n",
      "Epoch 108/150\n",
      "1474/1474 [==============================] - 642s 435ms/step - loss: 0.2072 - precision: 0.8239 - recall: 0.8470 - f1-score: 0.8085 - val_loss: 0.5699 - val_precision: 0.8902 - val_recall: 0.9196 - val_f1-score: 0.8494 - lr: 1.0000e-07\n",
      "Epoch 109/150\n",
      "1474/1474 [==============================] - 640s 434ms/step - loss: 0.2059 - precision: 0.8246 - recall: 0.8481 - f1-score: 0.8095 - val_loss: 0.5702 - val_precision: 0.8925 - val_recall: 0.9191 - val_f1-score: 0.8505 - lr: 1.0000e-07\n",
      "Epoch 110/150\n",
      "1474/1474 [==============================] - 639s 433ms/step - loss: 0.2070 - precision: 0.8240 - recall: 0.8479 - f1-score: 0.8085 - val_loss: 0.5706 - val_precision: 0.8958 - val_recall: 0.9186 - val_f1-score: 0.8526 - lr: 1.0000e-07\n",
      "Epoch 111/150\n",
      "1474/1474 [==============================] - 637s 432ms/step - loss: 0.2061 - precision: 0.8251 - recall: 0.8477 - f1-score: 0.8096 - val_loss: 0.5705 - val_precision: 0.8934 - val_recall: 0.9188 - val_f1-score: 0.8508 - lr: 1.0000e-07\n",
      "Epoch 112/150\n",
      "1474/1474 [==============================] - 639s 433ms/step - loss: 0.2060 - precision: 0.8248 - recall: 0.8483 - f1-score: 0.8100 - val_loss: 0.5704 - val_precision: 0.8952 - val_recall: 0.9186 - val_f1-score: 0.8521 - lr: 1.0000e-07\n",
      "Epoch 113/150\n",
      "1474/1474 [==============================] - 638s 433ms/step - loss: 0.2064 - precision: 0.8256 - recall: 0.8472 - f1-score: 0.8093 - val_loss: 0.5703 - val_precision: 0.8940 - val_recall: 0.9189 - val_f1-score: 0.8514 - lr: 1.0000e-07\n",
      "Epoch 114/150\n",
      "1474/1474 [==============================] - 639s 433ms/step - loss: 0.2064 - precision: 0.8240 - recall: 0.8478 - f1-score: 0.8092 - val_loss: 0.5703 - val_precision: 0.8927 - val_recall: 0.9192 - val_f1-score: 0.8506 - lr: 1.0000e-07\n",
      "Epoch 115/150\n",
      "1474/1474 [==============================] - 636s 431ms/step - loss: 0.2056 - precision: 0.8246 - recall: 0.8481 - f1-score: 0.8094 - val_loss: 0.5703 - val_precision: 0.8934 - val_recall: 0.9190 - val_f1-score: 0.8509 - lr: 1.0000e-07\n",
      "Epoch 116/150\n",
      "1474/1474 [==============================] - 634s 430ms/step - loss: 0.2074 - precision: 0.8234 - recall: 0.8466 - f1-score: 0.8078 - val_loss: 0.5707 - val_precision: 0.8957 - val_recall: 0.9184 - val_f1-score: 0.8525 - lr: 1.0000e-07\n",
      "Epoch 117/150\n",
      "1474/1474 [==============================] - 638s 433ms/step - loss: 0.2074 - precision: 0.8244 - recall: 0.8460 - f1-score: 0.8081 - val_loss: 0.5707 - val_precision: 0.8962 - val_recall: 0.9185 - val_f1-score: 0.8528 - lr: 1.0000e-07\n",
      "Epoch 118/150\n",
      "1474/1474 [==============================] - 636s 431ms/step - loss: 0.2060 - precision: 0.8249 - recall: 0.8470 - f1-score: 0.8087 - val_loss: 0.5706 - val_precision: 0.8943 - val_recall: 0.9187 - val_f1-score: 0.8514 - lr: 1.0000e-07\n",
      "Epoch 119/150\n",
      "1474/1474 [==============================] - 641s 435ms/step - loss: 0.2064 - precision: 0.8251 - recall: 0.8475 - f1-score: 0.8100 - val_loss: 0.5702 - val_precision: 0.8924 - val_recall: 0.9192 - val_f1-score: 0.8506 - lr: 1.0000e-07\n",
      "Epoch 120/150\n",
      "1474/1474 [==============================] - 638s 433ms/step - loss: 0.2065 - precision: 0.8236 - recall: 0.8473 - f1-score: 0.8075 - val_loss: 0.5701 - val_precision: 0.8916 - val_recall: 0.9193 - val_f1-score: 0.8502 - lr: 1.0000e-07\n",
      "Epoch 121/150\n",
      "1474/1474 [==============================] - 631s 428ms/step - loss: 0.2056 - precision: 0.8261 - recall: 0.8483 - f1-score: 0.8105 - val_loss: 0.5706 - val_precision: 0.8939 - val_recall: 0.9187 - val_f1-score: 0.8515 - lr: 1.0000e-07\n",
      "Epoch 122/150\n",
      "1474/1474 [==============================] - 633s 430ms/step - loss: 0.2057 - precision: 0.8239 - recall: 0.8490 - f1-score: 0.8093 - val_loss: 0.5703 - val_precision: 0.8933 - val_recall: 0.9192 - val_f1-score: 0.8509 - lr: 1.0000e-07\n",
      "Epoch 123/150\n",
      "1474/1474 [==============================] - 634s 430ms/step - loss: 0.2056 - precision: 0.8236 - recall: 0.8487 - f1-score: 0.8085 - val_loss: 0.5704 - val_precision: 0.8941 - val_recall: 0.9189 - val_f1-score: 0.8514 - lr: 1.0000e-07\n",
      "Epoch 124/150\n",
      "1474/1474 [==============================] - 637s 432ms/step - loss: 0.2070 - precision: 0.8250 - recall: 0.8457 - f1-score: 0.8078 - val_loss: 0.5708 - val_precision: 0.8968 - val_recall: 0.9183 - val_f1-score: 0.8534 - lr: 1.0000e-07\n",
      "Epoch 125/150\n",
      "1474/1474 [==============================] - 634s 430ms/step - loss: 0.2058 - precision: 0.8248 - recall: 0.8483 - f1-score: 0.8095 - val_loss: 0.5704 - val_precision: 0.8942 - val_recall: 0.9189 - val_f1-score: 0.8516 - lr: 1.0000e-07\n",
      "Epoch 126/150\n",
      "1474/1474 [==============================] - 642s 435ms/step - loss: 0.2058 - precision: 0.8242 - recall: 0.8487 - f1-score: 0.8097 - val_loss: 0.5703 - val_precision: 0.8941 - val_recall: 0.9188 - val_f1-score: 0.8515 - lr: 1.0000e-07\n",
      "Epoch 127/150\n",
      "1474/1474 [==============================] - 633s 430ms/step - loss: 0.2064 - precision: 0.8242 - recall: 0.8481 - f1-score: 0.8086 - val_loss: 0.5704 - val_precision: 0.8948 - val_recall: 0.9188 - val_f1-score: 0.8524 - lr: 1.0000e-07\n",
      "Epoch 128/150\n",
      "1474/1474 [==============================] - 633s 429ms/step - loss: 0.2057 - precision: 0.8246 - recall: 0.8476 - f1-score: 0.8086 - val_loss: 0.5702 - val_precision: 0.8933 - val_recall: 0.9192 - val_f1-score: 0.8513 - lr: 1.0000e-07\n",
      "Epoch 129/150\n",
      "1474/1474 [==============================] - 636s 432ms/step - loss: 0.2068 - precision: 0.8254 - recall: 0.8476 - f1-score: 0.8089 - val_loss: 0.5705 - val_precision: 0.8927 - val_recall: 0.9189 - val_f1-score: 0.8506 - lr: 1.0000e-07\n",
      "Epoch 130/150\n",
      "1474/1474 [==============================] - 637s 432ms/step - loss: 0.2056 - precision: 0.8248 - recall: 0.8483 - f1-score: 0.8101 - val_loss: 0.5703 - val_precision: 0.8933 - val_recall: 0.9189 - val_f1-score: 0.8510 - lr: 1.0000e-07\n",
      "Epoch 131/150\n",
      "1474/1474 [==============================] - 638s 433ms/step - loss: 0.2051 - precision: 0.8243 - recall: 0.8488 - f1-score: 0.8099 - val_loss: 0.5703 - val_precision: 0.8939 - val_recall: 0.9190 - val_f1-score: 0.8515 - lr: 1.0000e-07\n",
      "Epoch 132/150\n",
      "1474/1474 [==============================] - 636s 431ms/step - loss: 0.2051 - precision: 0.8261 - recall: 0.8491 - f1-score: 0.8101 - val_loss: 0.5706 - val_precision: 0.8952 - val_recall: 0.9185 - val_f1-score: 0.8522 - lr: 1.0000e-07\n",
      "Epoch 133/150\n",
      "1474/1474 [==============================] - 632s 429ms/step - loss: 0.2056 - precision: 0.8238 - recall: 0.8478 - f1-score: 0.8084 - val_loss: 0.5705 - val_precision: 0.8953 - val_recall: 0.9186 - val_f1-score: 0.8524 - lr: 1.0000e-07\n",
      "Epoch 134/150\n",
      "1474/1474 [==============================] - 634s 430ms/step - loss: 0.2057 - precision: 0.8243 - recall: 0.8476 - f1-score: 0.8085 - val_loss: 0.5703 - val_precision: 0.8934 - val_recall: 0.9191 - val_f1-score: 0.8514 - lr: 1.0000e-07\n",
      "Epoch 135/150\n",
      "1474/1474 [==============================] - 634s 430ms/step - loss: 0.2061 - precision: 0.8241 - recall: 0.8480 - f1-score: 0.8092 - val_loss: 0.5706 - val_precision: 0.8957 - val_recall: 0.9186 - val_f1-score: 0.8530 - lr: 1.0000e-07\n",
      "Epoch 136/150\n",
      "1474/1474 [==============================] - 635s 431ms/step - loss: 0.2050 - precision: 0.8250 - recall: 0.8491 - f1-score: 0.8100 - val_loss: 0.5704 - val_precision: 0.8926 - val_recall: 0.9188 - val_f1-score: 0.8503 - lr: 1.0000e-07\n",
      "Epoch 137/150\n",
      "1474/1474 [==============================] - 644s 436ms/step - loss: 0.2064 - precision: 0.8240 - recall: 0.8472 - f1-score: 0.8084 - val_loss: 0.5703 - val_precision: 0.8926 - val_recall: 0.9190 - val_f1-score: 0.8504 - lr: 1.0000e-07\n",
      "Epoch 138/150\n",
      "1474/1474 [==============================] - 636s 432ms/step - loss: 0.2057 - precision: 0.8243 - recall: 0.8483 - f1-score: 0.8093 - val_loss: 0.5703 - val_precision: 0.8932 - val_recall: 0.9190 - val_f1-score: 0.8508 - lr: 1.0000e-07\n",
      "Epoch 139/150\n",
      "1474/1474 [==============================] - 638s 433ms/step - loss: 0.2053 - precision: 0.8248 - recall: 0.8482 - f1-score: 0.8097 - val_loss: 0.5703 - val_precision: 0.8946 - val_recall: 0.9189 - val_f1-score: 0.8521 - lr: 1.0000e-07\n",
      "Epoch 140/150\n",
      "1474/1474 [==============================] - 632s 429ms/step - loss: 0.2056 - precision: 0.8247 - recall: 0.8484 - f1-score: 0.8097 - val_loss: 0.5708 - val_precision: 0.8963 - val_recall: 0.9186 - val_f1-score: 0.8533 - lr: 1.0000e-07\n",
      "Epoch 141/150\n",
      "1474/1474 [==============================] - 634s 430ms/step - loss: 0.2059 - precision: 0.8257 - recall: 0.8481 - f1-score: 0.8098 - val_loss: 0.5700 - val_precision: 0.8904 - val_recall: 0.9196 - val_f1-score: 0.8494 - lr: 1.0000e-07\n",
      "Epoch 142/150\n",
      "1474/1474 [==============================] - 634s 430ms/step - loss: 0.2067 - precision: 0.8242 - recall: 0.8471 - f1-score: 0.8086 - val_loss: 0.5703 - val_precision: 0.8937 - val_recall: 0.9189 - val_f1-score: 0.8512 - lr: 1.0000e-07\n",
      "Epoch 143/150\n",
      "1474/1474 [==============================] - 640s 434ms/step - loss: 0.2056 - precision: 0.8245 - recall: 0.8490 - f1-score: 0.8092 - val_loss: 0.5704 - val_precision: 0.8934 - val_recall: 0.9188 - val_f1-score: 0.8509 - lr: 1.0000e-07\n",
      "Epoch 144/150\n",
      "1474/1474 [==============================] - 633s 429ms/step - loss: 0.2063 - precision: 0.8246 - recall: 0.8472 - f1-score: 0.8086 - val_loss: 0.5705 - val_precision: 0.8942 - val_recall: 0.9186 - val_f1-score: 0.8513 - lr: 1.0000e-07\n",
      "Epoch 145/150\n",
      "1474/1474 [==============================] - 634s 430ms/step - loss: 0.2059 - precision: 0.8253 - recall: 0.8487 - f1-score: 0.8106 - val_loss: 0.5709 - val_precision: 0.8967 - val_recall: 0.9182 - val_f1-score: 0.8532 - lr: 1.0000e-07\n",
      "Epoch 146/150\n",
      "1474/1474 [==============================] - 636s 431ms/step - loss: 0.2058 - precision: 0.8251 - recall: 0.8484 - f1-score: 0.8090 - val_loss: 0.5704 - val_precision: 0.8938 - val_recall: 0.9189 - val_f1-score: 0.8512 - lr: 1.0000e-07\n",
      "Epoch 147/150\n",
      "1474/1474 [==============================] - 639s 434ms/step - loss: 0.2060 - precision: 0.8248 - recall: 0.8473 - f1-score: 0.8089 - val_loss: 0.5705 - val_precision: 0.8932 - val_recall: 0.9190 - val_f1-score: 0.8510 - lr: 1.0000e-07\n",
      "Epoch 148/150\n",
      "1474/1474 [==============================] - 635s 430ms/step - loss: 0.2058 - precision: 0.8239 - recall: 0.8478 - f1-score: 0.8081 - val_loss: 0.5705 - val_precision: 0.8949 - val_recall: 0.9187 - val_f1-score: 0.8517 - lr: 1.0000e-07\n",
      "Epoch 149/150\n",
      "1474/1474 [==============================] - 632s 429ms/step - loss: 0.2062 - precision: 0.8246 - recall: 0.8481 - f1-score: 0.8095 - val_loss: 0.5709 - val_precision: 0.8976 - val_recall: 0.9183 - val_f1-score: 0.8540 - lr: 1.0000e-07\n",
      "Epoch 150/150\n",
      "1474/1474 [==============================] - 634s 430ms/step - loss: 0.2067 - precision: 0.8253 - recall: 0.8474 - f1-score: 0.8096 - val_loss: 0.5701 - val_precision: 0.8917 - val_recall: 0.9192 - val_f1-score: 0.8498 - lr: 1.0000e-07\n",
      "test classification_report \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9837    0.9869    0.9853 601947346\n",
      "           1     0.9615    0.9572    0.9594 133718899\n",
      "           2     0.5902    0.4913    0.5362   8101819\n",
      "\n",
      "    accuracy                         0.9762 743768064\n",
      "   macro avg     0.8452    0.8118    0.8270 743768064\n",
      "weighted avg     0.9755    0.9762    0.9758 743768064\n",
      "\n",
      "train report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9864    0.9888    0.9876 1417509004\n",
      "           1     0.9658    0.9652    0.9655 303605705\n",
      "           2     0.6217    0.5078    0.5590  17833899\n",
      "\n",
      "    accuracy                         0.9798 1738948608\n",
      "   macro avg     0.8580    0.8206    0.8374 1738948608\n",
      "weighted avg     0.9791    0.9798    0.9794 1738948608\n",
      "\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(\n",
    "    train_dataloader, \n",
    "    steps_per_epoch=len(train_dataloader), \n",
    "    epochs= EPOCHS, \n",
    "    callbacks=callbacks, \n",
    "    validation_data=valid_dataloader, \n",
    "    validation_steps=len(valid_dataloader),\n",
    ")\n",
    "\n",
    "\n",
    "model.load_weights(model_name)\n",
    "\n",
    "direction = glob.glob(x_valid_dir+'*.png')\n",
    "\n",
    "dataset,d = valid_dataset, 'val_rgb/'\n",
    "\n",
    "preds,annos = [], []\n",
    "for i in range(len(dataset)):\n",
    "    \n",
    "    image, gt_mask = dataset[i] \n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    pr_mask = model.predict(image).round()\n",
    "    newimg = pr_mask[0,:,:,:]\n",
    "    name = direction[i].replace('test_rgb','test_anno')\n",
    "    \n",
    "    p = newimg[:,:,0]+newimg[:,:,1]*2+newimg[:,:,2]*0\n",
    "    \n",
    "    preds.append(p)\n",
    "    annos.append(cv2.imread(name)[:,:,0])\n",
    "    \n",
    "print('test classification_report ')    \n",
    "print(classification_report(np.array(annos).flatten(), np.array(preds).flatten(), labels=[0,1,2], digits = 4))\n",
    "\n",
    "\n",
    "direction = glob.glob(x_train_dir+'*.png')\n",
    "\n",
    "dataset,d = train_dataset, 'train_rgb/'\n",
    "preds,annos = [], []\n",
    "for i in range(len(dataset)):\n",
    "    \n",
    "    image, gt_mask = dataset[i] \n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    pr_mask = model.predict(image).round()\n",
    "    newimg = pr_mask[0,:,:,:]\n",
    "    name = direction[i].replace('train_rgb','train_anno')\n",
    "    \n",
    "    p = newimg[:,:,0]+newimg[:,:,1]*2+newimg[:,:,2]*0\n",
    "    preds.append(p)\n",
    "    annos.append(cv2.imread(name)[:,:,0])\n",
    "print('train report')    \n",
    "print(classification_report(np.array(annos).flatten(), np.array(preds).flatten(), labels=[0,1,2], digits = 4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
